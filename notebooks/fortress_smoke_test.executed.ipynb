{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33388062",
   "metadata": {},
   "source": [
    "# Setup and Environment\n",
    "This notebook serves production predictions for the Kaggle Hull Tactical - Market Prediction time-series API. It is optimized for fast startup, deterministic behavior, and robust logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bd3d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:46:47.080686Z",
     "iopub.status.busy": "2025-12-15T17:46:47.080351Z",
     "iopub.status.idle": "2025-12-15T17:46:50.933924Z",
     "shell.execute_reply": "2025-12-15T17:46:50.932015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] INFO - GPU enabled: False | Torch device: cpu | ONNX providers: ['CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] INFO - Model dir: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] INFO - Python: 3.13.5 | NumPy: 2.3.3 | Pandas: 2.3.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] INFO - scikit-learn: 1.7.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] INFO - LightGBM available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] INFO - PyTorch: 2.8.0+cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] INFO - Artifact dir: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset | Persist dir: \\kaggle\\working\n"
     ]
    }
   ],
   "source": [
    "# Imports, logging, seeds, thread limits, versions\n",
    "import os, sys, json, gc, time, logging, random, platform, warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Optional backends (only imported if artifacts are present)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "except Exception:\n",
    "    ort = None\n",
    "try:\n",
    "    import torch\n",
    "    torch.set_grad_enabled(False)\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# Reduce noisy sklearn pickle-version warnings\n",
    "try:\n",
    "    warnings.filterwarnings(\n",
    "        'ignore',\n",
    "        message='Trying to unpickle estimator StandardScaler',\n",
    "        category=UserWarning,\n",
    "        module='sklearn.base'\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Configure logger\n",
    "LOGGER = logging.getLogger('fortress')\n",
    "if not LOGGER.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    fmt = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s', datefmt='%H:%M:%S')\n",
    "    handler.setFormatter(fmt)\n",
    "    LOGGER.addHandler(handler)\n",
    "LOGGER.setLevel(os.getenv('FORTRESS_LOGLEVEL', 'INFO'))\n",
    "\n",
    "# Determinism and threads\n",
    "seed = int(os.getenv('SEED', '42'))\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ.setdefault('OMP_NUM_THREADS', os.getenv('OMP_NUM_THREADS', '2'))\n",
    "os.environ.setdefault('MKL_NUM_THREADS', os.getenv('MKL_NUM_THREADS', '2'))\n",
    "\n",
    "# GPU / device configuration (optional)\n",
    "USE_GPU = os.getenv('USE_GPU', '0') == '1'\n",
    "TORCH_DEVICE = 'cpu'\n",
    "ONNX_PROVIDERS = ['CPUExecutionProvider']\n",
    "if USE_GPU:\n",
    "    if torch is not None and torch.cuda.is_available():\n",
    "        TORCH_DEVICE = 'cuda'\n",
    "    if ort is not None:\n",
    "        try:\n",
    "            eps = ort.get_available_providers()\n",
    "            if 'CUDAExecutionProvider' in eps:\n",
    "                ONNX_PROVIDERS = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        except Exception:\n",
    "            pass\n",
    "LOGGER.info(f'GPU enabled: {USE_GPU} | Torch device: {TORCH_DEVICE} | ONNX providers: {ONNX_PROVIDERS}')\n",
    "\n",
    "# Paths\n",
    "KAGGLE_INPUT_DIR = Path('/kaggle/input')\n",
    "KAGGLE_WORK_DIR = Path('/kaggle/working')\n",
    "LOCAL_ROOT = Path.cwd()\n",
    "ARTIFACT_DIR = Path(os.getenv('ARTIFACT_DIR', '/kaggle/input/hull-champion-model-artifacts'))\n",
    "# Allow explicit model artifact dataset path (set via kernel env var `MODEL_ARTIFACTS_DIR`)\n",
    "MODEL_DIR = Path(os.getenv('MODEL_ARTIFACTS_DIR', os.getenv('ARTIFACT_DIR', '/kaggle/input/hull-champion-model-artifacts')))\n",
    "PERSIST_DIR = Path(os.getenv('PERSIST_DIR', KAGGLE_WORK_DIR))\n",
    "LOGGER.info(f'Model dir: {MODEL_DIR}')\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print versions/hardware\n",
    "LOGGER.info(f'Python: {platform.python_version()} | NumPy: {np.__version__} | Pandas: {pd.__version__}')\n",
    "try:\n",
    "    import sklearn\n",
    "    LOGGER.info(f'scikit-learn: {sklearn.__version__}')\n",
    "except Exception:\n",
    "    pass\n",
    "if lgb: LOGGER.info('LightGBM available')\n",
    "if ort: LOGGER.info(f'ONNX Runtime: {ort.__version__}')\n",
    "if torch: LOGGER.info(f'PyTorch: {torch.__version__}')\n",
    "LOGGER.info(f'Artifact dir: {ARTIFACT_DIR} | Persist dir: {PERSIST_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ae00e",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Artifacts\n",
    "Load persisted models/scalers and any configuration artifacts. Prefer the `experiments/backtest` outputs or Kaggle input datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b423fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:46:50.936367Z",
     "iopub.status.busy": "2025-12-15T17:46:50.935955Z",
     "iopub.status.idle": "2025-12-15T17:46:50.951924Z",
     "shell.execute_reply": "2025-12-15T17:46:50.951021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_model_fold1.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_model_fold2.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_model_fold3.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_model_fold4.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_model_fold5.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_scaler_fold1.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_scaler_fold2.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_scaler_fold3.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_scaler_fold4.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_scaler_fold5.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\optuna_risk_brain_best_params.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\risk_brain_full.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\risk_brain_scaler.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] WARNING - Missing artifact: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\risk_brain_full_meta.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:50] INFO - Artifacts resolved (Kaggle fixed paths).\n"
     ]
    }
   ],
   "source": [
    "# Resolve artifacts and load\n",
    "REPO_ROOT = LOCAL_ROOT\n",
    "if str(REPO_ROOT) not in sys.path: sys.path.insert(0, str(REPO_ROOT))\n",
    "# FortressPredictor will be defined inline below to avoid external imports on Kaggle\n",
    "\n",
    "# Primary structured layout under MODEL_DIR (allows overriding via kernel env vars)\n",
    "RET_MODELS = [MODEL_DIR / f'optuna_model_fold{i}.txt' for i in range(1,6)]\n",
    "RET_SCALERS = [MODEL_DIR / f'optuna_scaler_fold{i}.pkl' for i in range(1,6)]\n",
    "RISK_OPTUNA = MODEL_DIR / 'optuna_risk_brain_best_params.json'\n",
    "RISK_MODEL = MODEL_DIR / 'risk_brain_full.txt'\n",
    "RISK_SCALER = MODEL_DIR / 'risk_brain_scaler.pkl'\n",
    "RISK_META = MODEL_DIR / 'risk_brain_full_meta.json'\n",
    "\n",
    "# Validate presence (warn only; FortressPredictor expects these to exist on Kaggle)\n",
    "for p in RET_MODELS + RET_SCALERS + [RISK_OPTUNA, RISK_MODEL, RISK_SCALER, RISK_META]:\n",
    "    if not p.exists():\n",
    "        LOGGER.warning(f'Missing artifact: {p}')\n",
    "LOGGER.info('Artifacts resolved (Kaggle fixed paths).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f847d3",
   "metadata": {},
   "source": [
    "# Stateful Inference Engine\n",
    "We maintain a rolling window and produce one allocation per time step, reusing the FortressPredictor implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34245b",
   "metadata": {},
   "source": [
    "## Inline Fortress Predictor\n",
    "To ensure portability in Kaggle without repo imports, we inline the FortressPredictor and helpers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbac79d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:46:50.954239Z",
     "iopub.status.busy": "2025-12-15T17:46:50.953980Z",
     "iopub.status.idle": "2025-12-15T17:46:51.241680Z",
     "shell.execute_reply": "2025-12-15T17:46:51.240418Z"
    }
   },
   "outputs": [],
   "source": [
    "# FortressPredictor and helpers (inlined)\n",
    "from collections import deque\n",
    "from typing import Optional, Deque, Dict, Any, List\n",
    "import json as _json\n",
    "\n",
    "# LightGBM and joblib were imported above if available\n",
    "if lgb is None:\n",
    "    raise ImportError('LightGBM is required for inference. Please ensure it is available in Kaggle runtime.')\n",
    "\n",
    "# Import polars for on-the-fly feature building\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as _e:\n",
    "    pl = None\n",
    "\n",
    "# Small helpers that mirror the script implementation\n",
    "def _drop_and_select_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    drop_cols = ['row_id', 'time_id', 'date_id', 'date', 'market_forward_excess_returns']\n",
    "    fwd_cols = [c for c in df.columns if c.startswith('forward_')]\n",
    "    cols = [c for c in df.columns if c not in drop_cols and c not in fwd_cols]\n",
    "    return df[cols]\n",
    "\n",
    "def _safe_transform(X: pd.DataFrame, scaler: Any) -> pd.DataFrame:\n",
    "    \"\"\"Perform StandardScaler-like transform using stored stats when available to avoid sklearn version drift.\"\"\"\n",
    "    try:\n",
    "        mean_ = getattr(scaler, 'mean_', None)\n",
    "        scale_ = getattr(scaler, 'scale_', None)\n",
    "        if mean_ is not None and scale_ is not None:\n",
    "            mu = np.asarray(mean_, dtype=float)\n",
    "            sc = np.asarray(scale_, dtype=float)\n",
    "            if mu.shape[0] == X.shape[1] and sc.shape[0] == X.shape[1]:\n",
    "                Xn = (X.values.astype(float) - mu) / np.where(sc == 0, 1.0, sc)\n",
    "                return pd.DataFrame(Xn, columns=X.columns, index=X.index)\n",
    "        Xt = scaler.transform(X)\n",
    "        return pd.DataFrame(Xt, columns=X.columns, index=X.index)\n",
    "    except Exception:\n",
    "        return X.copy()\n",
    "\n",
    "def _numeric_cols_pl(df: 'pl.DataFrame', exclude: List[str] = None) -> List[str]:\n",
    "    if exclude is None:\n",
    "        exclude = ['row_id', 'time_id', 'date_id', 'date', 'market_forward_excess_returns']\n",
    "    numeric_kinds = ('Int', 'Float', 'Decimal')\n",
    "    out = []\n",
    "    for c, t in zip(df.columns, df.dtypes):\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        if any(k in str(t) for k in numeric_kinds):\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def _make_lag_features(df: 'pl.DataFrame', cols: List[str], lags: List[int]) -> 'pl.DataFrame':\n",
    "    out = df\n",
    "    for c in cols:\n",
    "        for l in lags:\n",
    "            out = out.with_columns(pl.col(c).shift(l).alias(f\"{c}_lag{l}\"))\n",
    "    return out\n",
    "\n",
    "def _make_rolling_features(df: 'pl.DataFrame', cols: List[str], windows: List[int], aggs=(\"mean\",\"std\")) -> 'pl.DataFrame':\n",
    "    out = df\n",
    "    for c in cols:\n",
    "        for w in windows:\n",
    "            for agg in aggs:\n",
    "                if agg == 'mean':\n",
    "                    out = out.with_columns(pl.col(c).rolling_mean(window_size=w).alias(f\"{c}_r{w}_mean\"))\n",
    "                elif agg == 'std':\n",
    "                    out = out.with_columns(pl.col(c).rolling_std(window_size=w).alias(f\"{c}_r{w}_std\"))\n",
    "                elif agg == 'sum':\n",
    "                    out = out.with_columns(pl.col(c).rolling_sum(window_size=w).alias(f\"{c}_r{w}_sum\"))\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown agg {agg}\")\n",
    "    return out\n",
    "\n",
    "def _prefix_group_aggregations(df: 'pl.DataFrame', prefixes: List[str], windows: List[int]) -> 'pl.DataFrame':\n",
    "    out = df\n",
    "    cols = df.columns\n",
    "    from functools import reduce\n",
    "    import operator\n",
    "    numeric_kinds = ('Int', 'Float', 'Decimal')\n",
    "    for p in prefixes:\n",
    "        group_cols = [c for c in cols if c.startswith(p)]\n",
    "        group_cols = [c for c in group_cols if any(k in str(out.schema.get(c)) for k in numeric_kinds)]\n",
    "        if not group_cols:\n",
    "            continue\n",
    "        expr_sum = reduce(operator.add, [pl.col(c) for c in group_cols])\n",
    "        expr = expr_sum / len(group_cols)\n",
    "        out = out.with_columns(expr.alias(f'G_{p}_mean'))\n",
    "        for w in windows:\n",
    "            out = out.with_columns(pl.col(f'G_{p}_mean').rolling_mean(window_size=w).alias(f'G_{p}_r{w}_mean'))\n",
    "            out = out.with_columns(pl.col(f'G_{p}_mean').rolling_std(window_size=w).alias(f'G_{p}_r{w}_std'))\n",
    "    return out\n",
    "\n",
    "class RollingBuffer:\n",
    "    def __init__(self, maxlen: int = 252):\n",
    "        self.maxlen = maxlen\n",
    "        self._buf: Deque[pd.Series] = deque(maxlen=maxlen)\n",
    "        self.last_time_id: Optional[int] = None\n",
    "    def update(self, row: Dict[str, Any] | pd.Series):\n",
    "        if isinstance(row, dict):\n",
    "            row = pd.Series(row)\n",
    "        # Optional chronological enforcement via env var ENFORCE_CHRONO\n",
    "        try:\n",
    "            if os.getenv('ENFORCE_CHRONO', '0') == '1' and 'time_id' in row:\n",
    "                tid = int(row.get('time_id'))\n",
    "                if self.last_time_id is not None and tid < self.last_time_id:\n",
    "                    LOGGER.warning(f'Chronology violation: incoming time_id {tid} < last {self.last_time_id}; skipping update')\n",
    "                    return\n",
    "                self.last_time_id = tid\n",
    "        except Exception:\n",
    "            pass\n",
    "        self._buf.append(row)\n",
    "    def to_pandas(self) -> pd.DataFrame:\n",
    "        if not self._buf:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self._buf).reset_index(drop=True)\n",
    "    def build_features(self) -> pd.DataFrame:\n",
    "        if pl is None:\n",
    "            # Fallback: without polars we cannot build rich features; return current buffer as-is\n",
    "            return self.to_pandas()\n",
    "        pdf = self.to_pandas()\n",
    "        if pdf.empty:\n",
    "            return pdf\n",
    "        pl_df = pl.from_pandas(pdf)\n",
    "        num_cols = _numeric_cols_pl(pl_df)\n",
    "        lags = [1, 2, 3]\n",
    "        windows = [5, 21]\n",
    "        pl_feat = _make_lag_features(pl_df, num_cols, lags)\n",
    "        pl_feat = _make_rolling_features(pl_feat, num_cols, windows, aggs=(\"mean\",\"std\"))\n",
    "        prefixes = ['M','E','P','V','S','MOM','D','I']\n",
    "        pl_feat = _prefix_group_aggregations(pl_feat, prefixes, windows)\n",
    "        return pl_feat.to_pandas()\n",
    "\n",
    "class DynamicScalerState:\n",
    "    def __init__(self, alpha: float, clip_lo: float, clip_hi: float, target_annual_vol: float, max_daily_scale_change: Optional[float], alloc_cap: Optional[float]):\n",
    "        self.alpha = alpha\n",
    "        self.clip_lo = clip_lo\n",
    "        self.clip_hi = clip_hi\n",
    "        self.target_daily = target_annual_vol / np.sqrt(252.0)\n",
    "        self.msc = max_daily_scale_change\n",
    "        self.alloc_cap = alloc_cap\n",
    "        self.s_vol_prev: Optional[float] = None\n",
    "        self.scale_prev: Optional[float] = None\n",
    "        self.alloc_prev: Optional[float] = None\n",
    "    def step(self, base_alloc: float, predicted_vol_daily: float) -> float:\n",
    "        if self.s_vol_prev is None:\n",
    "            s_vol = predicted_vol_daily\n",
    "        else:\n",
    "            s_vol = self.alpha * predicted_vol_daily + (1 - self.alpha) * self.s_vol_prev\n",
    "        self.s_vol_prev = s_vol\n",
    "        scale = self.target_daily / max(s_vol, 1e-6)\n",
    "        scale = float(np.clip(scale, self.clip_lo, self.clip_hi))\n",
    "        if self.msc is not None and self.scale_prev is not None:\n",
    "            lo = self.scale_prev * (1.0 - self.msc)\n",
    "            hi = self.scale_prev * (1.0 + self.msc)\n",
    "            scale = float(np.clip(scale, lo, hi))\n",
    "        self.scale_prev = scale\n",
    "        alloc = base_alloc * scale\n",
    "        if self.alloc_prev is not None and self.alloc_cap is not None:\n",
    "            lo = self.alloc_prev - self.alloc_cap\n",
    "            hi = self.alloc_prev + self.alloc_cap\n",
    "            alloc = float(np.clip(alloc, lo, hi))\n",
    "        self.alloc_prev = alloc\n",
    "        return alloc\n",
    "\n",
    "class FortressPredictor:\n",
    "    def __init__(self, buffer_len: int = 252, base_scale: float = 0.5):\n",
    "        self.buffer = RollingBuffer(maxlen=buffer_len)\n",
    "        self.base_scale = base_scale\n",
    "        self._ret_hist: Deque[float] = deque(maxlen=buffer_len)\n",
    "        self._ret_clip = (0.0, 2.0)\n",
    "        self.last_diag: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        # Load return ensemble boosters + scalers from Kaggle input artifacts\n",
    "        self.ret_models: List[Any] = []\n",
    "        self.ret_scalers: List[Any] = []\n",
    "        self.ret_feat_names: List[List[str]] = []\n",
    "        for mpath, spath in zip(RET_MODELS, RET_SCALERS):\n",
    "            if mpath.exists() and spath.exists():\n",
    "                self.ret_models.append(lgb.Booster(model_file=str(mpath)))\n",
    "                self.ret_scalers.append(joblib.load(spath))\n",
    "                self.ret_feat_names.append(self.ret_models[-1].feature_name())\n",
    "                # Persist scaler stats to JSON to avoid sklearn pickle drift at inference\n",
    "                try:\n",
    "                    scaler = self.ret_scalers[-1]\n",
    "                    mean_ = getattr(scaler, 'mean_', None)\n",
    "                    scale_ = getattr(scaler, 'scale_', None)\n",
    "                    if mean_ is not None and scale_ is not None:\n",
    "                        stats = {'mean': [float(x) for x in mean_], 'scale': [float(x) for x in scale_]}\n",
    "                        outp = PERSIST_DIR / f'ret_scaler_stats_fold{len(self.ret_scalers)}.json'\n",
    "                        with open(outp, 'w') as _f:\n",
    "                            _json.dump(stats, _f)\n",
    "                        LOGGER.info(f'Wrote scaler stats to {outp}')\n",
    "                except Exception as _:\n",
    "                    pass\n",
    "        if not self.ret_models:\n",
    "            raise FileNotFoundError(f'Return model boosters/scalers not found under {MODEL_DIR}')\n",
    "\n",
    "        # Load risk brain\n",
    "        self.knobs = {}\n",
    "        try:\n",
    "            with open(RISK_OPTUNA, 'r') as f:\n",
    "                rb = _json.load(f)\n",
    "            self.knobs = rb.get('best_params', rb)\n",
    "        except Exception:\n",
    "            self.knobs = {}\n",
    "        self.risk_target_kind = self.knobs.get('risk_target_kind', 'sq')\n",
    "\n",
    "        self.risk_model = None\n",
    "        self.risk_scaler = None\n",
    "        self.risk_feat_names: List[str] = []\n",
    "        self.calib_ratio = 1.0\n",
    "        if RISK_MODEL.exists() and RISK_SCALER.exists() and RISK_META.exists():\n",
    "            self.risk_model = lgb.Booster(model_file=str(RISK_MODEL))\n",
    "            self.risk_scaler = joblib.load(RISK_SCALER)\n",
    "            self.risk_feat_names = self.risk_model.feature_name()\n",
    "            try:\n",
    "                with open(RISK_META, 'r') as f:\n",
    "                    meta = _json.load(f)\n",
    "                self.calib_ratio = float(meta.get('calib_ratio', 1.0))\n",
    "            except Exception:\n",
    "                self.calib_ratio = 1.0\n",
    "            # Persist risk scaler stats as JSON\n",
    "            try:\n",
    "                mean_ = getattr(self.risk_scaler, 'mean_', None)\n",
    "                scale_ = getattr(self.risk_scaler, 'scale_', None)\n",
    "                if mean_ is not None and scale_ is not None:\n",
    "                    stats = {'mean': [float(x) for x in mean_], 'scale': [float(x) for x in scale_]}\n",
    "                    outp = PERSIST_DIR / 'risk_scaler_stats.json'\n",
    "                    with open(outp, 'w') as _f:\n",
    "                        _json.dump(stats, _f)\n",
    "                    LOGGER.info(f'Wrote risk scaler stats to {outp}')\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            raise FileNotFoundError(f'Risk brain artifacts not found under {MODEL_DIR}')\n",
    "\n",
    "        self.scaler_state = DynamicScalerState(\n",
    "            alpha=float(self.knobs.get('alpha', 0.2)),\n",
    "            clip_lo=float(self.knobs.get('clip_lo', 0.25)),\n",
    "            clip_hi=float(self.knobs.get('clip_hi', 2.5)),\n",
    "            target_annual_vol=float(self.knobs.get('target_annual_vol', 0.12)),\n",
    "            max_daily_scale_change=float(self.knobs.get('max_daily_scale_change', 0.3)),\n",
    "            alloc_cap=(None if self.knobs.get('alloc_cap', None) is None else float(self.knobs['alloc_cap'])),\n",
    ")\n",
    "\n",
    "    def _align_columns(self, X: pd.DataFrame, feature_names: List[str]) -> pd.DataFrame:\n",
    "        present = set(X.columns)\n",
    "        missing = [c for c in feature_names if c not in present]\n",
    "        if missing:\n",
    "            for c in missing:\n",
    "                X[c] = 0.0\n",
    "        return X[feature_names]\n",
    "\n",
    "    def _has_all_features(self, df_cols: List[str]) -> bool:\n",
    "        if not self.ret_feat_names or not self.risk_feat_names:\n",
    "            return False\n",
    "        ret_ok = all(c in df_cols for c in self.ret_feat_names[0])\n",
    "        risk_ok = all(c in df_cols for c in self.risk_feat_names)\n",
    "        return ret_ok and risk_ok\n",
    "\n",
    "    def update_and_predict(self, day_row: Dict[str, Any] | pd.Series, expects_features: bool = True) -> float:\n",
    "        self.buffer.update(day_row)\n",
    "        # Auto-detect if the incoming rows are raw or already featured\n",
    "        feat_df = self.buffer.to_pandas()\n",
    "        if feat_df.empty:\n",
    "            return 0.0\n",
    "        if not self._has_all_features(list(feat_df.columns)) or not expects_features:\n",
    "            feat_df = self.buffer.build_features()\n",
    "            if feat_df.empty:\n",
    "                return 0.0\n",
    "        X_all = _drop_and_select_features(feat_df)\n",
    "        x_today = X_all.tail(1).copy()\n",
    "        ret_preds = []\n",
    "        for booster, scaler, feat_names in zip(self.ret_models, self.ret_scalers, self.ret_feat_names):\n",
    "            Xt = self._align_columns(x_today.copy(), feat_names)\n",
    "            Xt_s = _safe_transform(Xt, scaler)\n",
    "            ret_preds.append(float(booster.predict(Xt_s)[0]))\n",
    "        pred_ret = float(np.mean(ret_preds))\n",
    "        self._ret_hist.append(pred_ret)\n",
    "        arr = np.array(self._ret_hist, dtype=float)\n",
    "        if len(arr) >= 5:\n",
    "            mean = float(np.nanmean(arr))\n",
    "            std = float(np.nanstd(arr))\n",
    "            z = 0.0 if std <= 1e-12 or np.isnan(std) else (pred_ret - mean) / std\n",
    "        else:\n",
    "            z = 0.0\n",
    "        mid = (self._ret_clip[0] + self._ret_clip[1]) / 2.0\n",
    "        width = (self._ret_clip[1] - self._ret_clip[0]) / 2.0\n",
    "        alloc_base = mid + width * np.tanh(self.base_scale * z)\n",
    "\n",
    "        Xr = self._align_columns(x_today.copy(), self.risk_feat_names)\n",
    "        Xr_s = _safe_transform(Xr, self.risk_scaler)\n",
    "        pr = float(self.risk_model.predict(Xr_s)[0])\n",
    "        if self.risk_target_kind == 'sq':\n",
    "            pred_vol = float(np.sqrt(max(pr, 0.0) + 1e-12))\n",
    "        elif self.risk_target_kind == 'abs':\n",
    "            pred_vol = float(np.clip(pr, 0.0, None))\n",
    "        else:\n",
    "            pred_vol = float(np.exp(pr / 2.0))\n",
    "        pred_vol *= self.calib_ratio\n",
    "\n",
    "        alloc_final = self.scaler_state.step(base_alloc=alloc_base, predicted_vol_daily=pred_vol)\n",
    "        alloc_final = float(np.clip(alloc_final, 0.0, 2.0))\n",
    "\n",
    "        self.last_diag = {\n",
    "            'pred_ret': float(pred_ret),\n",
    "            'alloc_base': float(alloc_base),\n",
    "            'pred_vol': float(pred_vol),\n",
    "            'smoothed_vol': float(self.scaler_state.s_vol_prev if self.scaler_state.s_vol_prev is not None else pred_vol),\n",
    "            'scale': float(self.scaler_state.scale_prev if self.scaler_state.scale_prev is not None else 1.0),\n",
    "            'alloc_final': float(alloc_final),\n",
    "        }\n",
    "        return float(alloc_final)\n",
    "\n",
    "class FortressAPI:\n",
    "    def __init__(self):\n",
    "        self.model = FortressPredictor()\n",
    "        self.use_gpu = os.getenv('USE_GPU', '0') == '1'\n",
    "        self.torch_device = 'cuda' if self.use_gpu and (torch is not None) and torch.cuda.is_available() else 'cpu'\n",
    "        # Diagnostics collection for reporting\n",
    "        self.collect_diag = os.getenv('COLLECT_DIAGNOSTICS', '1') == '1'\n",
    "        self._allocations: List[float] = []\n",
    "        self._pred_returns: List[float] = []\n",
    "        self._pred_vols: List[float] = []\n",
    "        self._smoothed_vols: List[float] = []\n",
    "        self._scales: List[float] = []\n",
    "        self._last_predict_seconds: float = 0.0\n",
    "        \n",
    "    def reset_diags(self):\n",
    "        self._allocations.clear()\n",
    "        self._pred_returns.clear()\n",
    "        self._pred_vols.clear()\n",
    "        self._smoothed_vols.clear()\n",
    "        self._scales.clear()\n",
    "        self._last_predict_seconds = 0.0\n",
    "    \n",
    "    def predict_batch(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Accepts a pandas DataFrame batch; emits a 1D np.ndarray of allocations (float32).\n",
    "        This function advances internal state row-by-row to preserve chronology.\"\"\"\n",
    "        if self.collect_diag:\n",
    "            self.reset_diags()\n",
    "        t0 = time.perf_counter()\n",
    "        out = np.empty(len(df), dtype=np.float32)\n",
    "        max_seconds = float(os.getenv('MAX_BATCH_SECONDS', '300'))\n",
    "        for i, (_, row) in enumerate(df.iterrows()):\n",
    "            # Timeout check\n",
    "            if (time.perf_counter() - t0) > max_seconds:\n",
    "                LOGGER.warning(f'predict_batch timeout ({max_seconds}s) reached at index {i}; filling remaining outputs with last value')\n",
    "                last_val = float(out[i-1]) if i > 0 else 0.0\n",
    "                out[i:] = last_val\n",
    "                break\n",
    "            out[i] = self.model.update_and_predict(row, expects_features=True)\n",
    "            if self.collect_diag:\n",
    "                diag = self.model.last_diag or {}\n",
    "                self._allocations.append(float(out[i]))\n",
    "                self._pred_returns.append(float(diag.get('pred_ret', np.nan)))\n",
    "                self._pred_vols.append(float(diag.get('pred_vol', np.nan)))\n",
    "                self._smoothed_vols.append(float(diag.get('smoothed_vol', np.nan)))\n",
    "                self._scales.append(float(diag.get('scale', np.nan)))\n",
    "        self._last_predict_seconds = time.perf_counter() - t0\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a3d46e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:46:51.244206Z",
     "iopub.status.busy": "2025-12-15T17:46:51.243941Z",
     "iopub.status.idle": "2025-12-15T17:46:51.251804Z",
     "shell.execute_reply": "2025-12-15T17:46:51.250648Z"
    }
   },
   "outputs": [],
   "source": [
    "# No additional engine code needed: FortressPredictor encapsulates state, feature transforms, and dynamic sizing.\n",
    "# We only define a thin wrapper to align with Kaggle's expected interface.\n",
    "class FortressAPI:\n",
    "    def __init__(self):\n",
    "        self.model = FortressPredictor()\n",
    "        self.use_gpu = os.getenv('USE_GPU', '0') == '1'\n",
    "        self.torch_device = 'cuda' if self.use_gpu and (torch is not None) and torch.cuda.is_available() else 'cpu'\n",
    "        # Diagnostics collection for reporting\n",
    "        self.collect_diag = os.getenv('COLLECT_DIAGNOSTICS', '1') == '1'\n",
    "        self._allocations: List[float] = []\n",
    "        self._pred_returns: List[float] = []\n",
    "        self._pred_vols: List[float] = []\n",
    "        self._smoothed_vols: List[float] = []\n",
    "        self._scales: List[float] = []\n",
    "        self._last_predict_seconds: float = 0.0\n",
    "        \n",
    "    def reset_diags(self):\n",
    "        self._allocations.clear()\n",
    "        self._pred_returns.clear()\n",
    "        self._pred_vols.clear()\n",
    "        self._smoothed_vols.clear()\n",
    "        self._scales.clear()\n",
    "        self._last_predict_seconds = 0.0\n",
    "    \n",
    "    def predict_batch(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Accepts a pandas DataFrame batch; emits a 1D np.ndarray of allocations (float32).\n",
    "        This function advances internal state row-by-row to preserve chronology.\"\"\"\n",
    "        if self.collect_diag:\n",
    "            self.reset_diags()\n",
    "        t0 = time.perf_counter()\n",
    "        out = np.empty(len(df), dtype=np.float32)\n",
    "        for i, (_, row) in enumerate(df.iterrows()):\n",
    "            out[i] = self.model.update_and_predict(row, expects_features=True)\n",
    "            if self.collect_diag:\n",
    "                diag = self.model.last_diag or {}\n",
    "                self._allocations.append(float(out[i]))\n",
    "                self._pred_returns.append(float(diag.get('pred_ret', np.nan)))\n",
    "                self._pred_vols.append(float(diag.get('pred_vol', np.nan)))\n",
    "                self._smoothed_vols.append(float(diag.get('smoothed_vol', np.nan)))\n",
    "                self._scales.append(float(diag.get('scale', np.nan)))\n",
    "        self._last_predict_seconds = time.perf_counter() - t0\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da706a",
   "metadata": {},
   "source": [
    "# Model Instantiation\n",
    "Create a single global instance of the API adapter to maintain state across calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b73cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:46:51.255081Z",
     "iopub.status.busy": "2025-12-15T17:46:51.254562Z",
     "iopub.status.idle": "2025-12-15T17:46:51.263415Z",
     "shell.execute_reply": "2025-12-15T17:46:51.262270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:51] WARNING - Failed to instantiate FortressAPI: Return model boosters/scalers not found under G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset; using DummyFortress for local run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:51] INFO - DummyFortress instantiated for local smoke-run.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate once (with fallback to a dummy predictor for local smoke-runs)\n",
    "try:\n",
    "    FORTRESS = FortressAPI()\n",
    "    LOGGER.info('FortressAPI instantiated and ready.')\n",
    "except Exception as e:\n",
    "    LOGGER.warning(f'Failed to instantiate FortressAPI: {e}; using DummyFortress for local run')\n",
    "    class DummyFortress:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def predict_batch(self, df: pd.DataFrame) -> np.ndarray:\n",
    "            LOGGER.warning('DummyFortress.predict_batch called — returning zeros')\n",
    "            return np.zeros(len(df), dtype=np.float32)\n",
    "    FORTRESS = DummyFortress()\n",
    "    LOGGER.info('DummyFortress instantiated for local smoke-run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6862ac45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:46:51.266107Z",
     "iopub.status.busy": "2025-12-15T17:46:51.265665Z",
     "iopub.status.idle": "2025-12-15T17:46:51.273516Z",
     "shell.execute_reply": "2025-12-15T17:46:51.272428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:16:51] INFO - Warmup file found at: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\warmup_features.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup file present: G:\\2025\\Kaggle\\#3.Hull Tactical - Market Prediction\\dataset\\warmup_features.parquet\n"
     ]
    }
   ],
   "source": [
    "# Warmup presence check - aborts with instructions if warmup missing\n",
    "warmup_candidates = [\n",
    "    ARTIFACT_DIR / 'warmup_features.parquet',\n",
    "    LOCAL_ROOT / 'data' / 'processed' / 'features_post_gfc.parquet',\n",
    "    LOCAL_ROOT / 'data' / 'processed' / 'features_post_gfc_sample.csv',\n",
    "]\n",
    "warmup_path = None\n",
    "for p in warmup_candidates:\n",
    "    try:\n",
    "        if p.exists():\n",
    "            warmup_path = p\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "if warmup_path is None:\n",
    "    msg = (\n",
    "        'Warm-up file not found. For Kaggle submission, upload the precomputed ',\n",
    "        'warmup_features.parquet as a dataset input named \"warmup-features\" and ',\n",
    "        'set ARTIFACT_DIR to /kaggle/input/warmup-features. Alternatively, run ',\n",
    "        'scripts/package_warmup_dataset.py to produce a local packaging directory.'\n",
    "    )\n",
    "    msg = ''.join(msg)\n",
    "    LOGGER.error(msg)\n",
    "    print(msg)\n",
    "    # Halt further execution in notebook run to avoid cold-start failures\n",
    "    raise FileNotFoundError(msg)\n",
    "else:\n",
    "    LOGGER.info(f'Warmup file found at: {warmup_path}')\n",
    "    print('Warmup file present:', warmup_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c767fd88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:46:51.276259Z",
     "iopub.status.busy": "2025-12-15T17:46:51.275700Z",
     "iopub.status.idle": "2025-12-15T17:46:51.337103Z",
     "shell.execute_reply": "2025-12-15T17:46:51.335517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded warmup rows: 8\n",
      "[23:16:51] WARNING - DummyFortress.predict_batch called — returning zeros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocations: [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Smoke test: load a small warmup sample and run FORTRESS.predict_batch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "p = Path('dataset') / 'warmup_features.parquet'\n",
    "if not p.exists():\n",
    "    p = Path('/kaggle/input/warmup-features') / 'warmup_features.parquet'\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(f'Warmup file not found at {p}')\n",
    "df = pd.read_parquet(p).head(8)\n",
    "print('Loaded warmup rows:', len(df))\n",
    "allocs = FORTRESS.predict_batch(df)\n",
    "print('Allocations:', allocs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
