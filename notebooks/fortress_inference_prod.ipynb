{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33388062",
   "metadata": {},
   "source": [
    "# Setup and Environment\n",
    "This notebook serves production predictions for the Kaggle Hull Tactical - Market Prediction time-series API. It is optimized for fast startup, deterministic behavior, and robust logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, logging, seeds, thread limits, versions\n",
    "import os, sys, json, gc, time, logging, random, platform, warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Optional backends (only imported if artifacts are present)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "except Exception:\n",
    "    ort = None\n",
    "try:\n",
    "    import torch\n",
    "    torch.set_grad_enabled(False)\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# Reduce noisy sklearn pickle-version warnings\n",
    "try:\n",
    "    warnings.filterwarnings(\n",
    "        'ignore',\n",
    "        message='Trying to unpickle estimator StandardScaler',\n",
    "        category=UserWarning,\n",
    "        module='sklearn.base'\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Configure logger\n",
    "LOGGER = logging.getLogger('fortress')\n",
    "if not LOGGER.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    fmt = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s', datefmt='%H:%M:%S')\n",
    "    handler.setFormatter(fmt)\n",
    "    LOGGER.addHandler(handler)\n",
    "LOGGER.setLevel(os.getenv('FORTRESS_LOGLEVEL', 'INFO'))\n",
    "\n",
    "# Determinism and threads\n",
    "seed = int(os.getenv('SEED', '42'))\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ.setdefault('OMP_NUM_THREADS', os.getenv('OMP_NUM_THREADS', '2'))\n",
    "os.environ.setdefault('MKL_NUM_THREADS', os.getenv('MKL_NUM_THREADS', '2'))\n",
    "\n",
    "# GPU / device configuration (optional)\n",
    "USE_GPU = os.getenv('USE_GPU', '0') == '1'\n",
    "TORCH_DEVICE = 'cpu'\n",
    "ONNX_PROVIDERS: List[str] = ['CPUExecutionProvider']\n",
    "if USE_GPU:\n",
    "    if torch is not None and torch.cuda.is_available():\n",
    "        TORCH_DEVICE = 'cuda'\n",
    "    if ort is not None:\n",
    "        try:\n",
    "            eps = ort.get_available_providers()\n",
    "            if 'CUDAExecutionProvider' in eps:\n",
    "                ONNX_PROVIDERS = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        except Exception:\n",
    "            pass\n",
    "LOGGER.info(f'GPU enabled: {USE_GPU} | Torch device: {TORCH_DEVICE} | ONNX providers: {ONNX_PROVIDERS}')\n",
    "\n",
    "# Paths\n",
    "KAGGLE_INPUT_DIR = Path('/kaggle/input')\n",
    "KAGGLE_WORK_DIR = Path('/kaggle/working')\n",
    "LOCAL_ROOT = Path.cwd()\n",
    "ARTIFACT_DIR = Path(os.getenv('ARTIFACT_DIR', '/kaggle/input/hull-champion-model-artifacts'))\n",
    "PERSIST_DIR = Path(os.getenv('PERSIST_DIR', KAGGLE_WORK_DIR))\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print versions/hardware\n",
    "LOGGER.info(f'Python: {platform.python_version()} | NumPy: {np.__version__} | Pandas: {pd.__version__}')\n",
    "try:\n",
    "    import sklearn\n",
    "    LOGGER.info(f'scikit-learn: {sklearn.__version__}')\n",
    "except Exception:\n",
    "    pass\n",
    "if lgb: LOGGER.info('LightGBM available')\n",
    "if ort: LOGGER.info(f'ONNX Runtime: {ort.__version__}')\n",
    "if torch: LOGGER.info(f'PyTorch: {torch.__version__}')\n",
    "LOGGER.info(f'Artifact dir: {ARTIFACT_DIR} | Persist dir: {PERSIST_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ae00e",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Artifacts\n",
    "Load persisted models/scalers and any configuration artifacts. Prefer the `experiments/backtest` outputs or Kaggle input datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b423fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve artifacts and load\n",
    "REPO_ROOT = LOCAL_ROOT\n",
    "if str(REPO_ROOT) not in sys.path: sys.path.insert(0, str(REPO_ROOT))\n",
    "# FortressPredictor will be defined inline below to avoid external imports on Kaggle\n",
    "\n",
    "# Primary structured layout under ARTIFACT_DIR\n",
    "RET_MODELS = [Path('/kaggle/input/hull-champion-model-artifacts') / f'optuna_model_fold{i}.txt' for i in range(1,6)]\n",
    "RET_SCALERS = [Path('/kaggle/input/hull-champion-model-artifacts') / f'optuna_scaler_fold{i}.pkl' for i in range(1,6)]\n",
    "RISK_OPTUNA = Path('/kaggle/input/hull-champion-model-artifacts') / 'optuna_risk_brain_best_params.json'\n",
    "RISK_MODEL = Path('/kaggle/input/hull-champion-model-artifacts') / 'risk_brain_full.txt'\n",
    "RISK_SCALER = Path('/kaggle/input/hull-champion-model-artifacts') / 'risk_brain_scaler.pkl'\n",
    "RISK_META = Path('/kaggle/input/hull-champion-model-artifacts') / 'risk_brain_full_meta.json'\n",
    "\n",
    "# Validate presence (warn only; FortressPredictor expects these to exist on Kaggle)\n",
    "for p in RET_MODELS + RET_SCALERS + [RISK_OPTUNA, RISK_MODEL, RISK_SCALER, RISK_META]:\n",
    "    if not p.exists():\n",
    "        LOGGER.warning(f'Missing artifact: {p}')\n",
    "LOGGER.info('Artifacts resolved (Kaggle fixed paths).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f847d3",
   "metadata": {},
   "source": [
    "# Stateful Inference Engine\n",
    "We maintain a rolling window and produce one allocation per time step, reusing the FortressPredictor implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34245b",
   "metadata": {},
   "source": [
    "## Inline Fortress Predictor\n",
    "To ensure portability in Kaggle without repo imports, we inline the FortressPredictor and helpers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FortressPredictor and helpers (inlined)\n",
    "from collections import deque\n",
    "from typing import Optional, Deque, Dict, Any, List\n",
    "import json as _json\n",
    "\n",
    "# LightGBM and joblib were imported above if available\n",
    "if lgb is None:\n",
    "    raise ImportError('LightGBM is required for inference. Please ensure it is available in Kaggle runtime.')\n",
    "\n",
    "# Import polars for on-the-fly feature building\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception as _e:\n",
    "    pl = None\n",
    "\n",
    "# Small helpers that mirror the script implementation\n",
    "def _drop_and_select_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    drop_cols = ['row_id', 'time_id', 'date_id', 'date', 'market_forward_excess_returns']\n",
    "    fwd_cols = [c for c in df.columns if c.startswith('forward_')]\n",
    "    cols = [c for c in df.columns if c not in drop_cols and c not in fwd_cols]\n",
    "    return df[cols]\n",
    "\n",
    "def _safe_transform(X: pd.DataFrame, scaler: Any) -> pd.DataFrame:\n",
    "    \"\"\"Perform StandardScaler-like transform using stored stats when available to avoid sklearn version drift.\"\"\"\n",
    "    try:\n",
    "        mean_ = getattr(scaler, 'mean_', None)\n",
    "        scale_ = getattr(scaler, 'scale_', None)\n",
    "        if mean_ is not None and scale_ is not None:\n",
    "            mu = np.asarray(mean_, dtype=float)\n",
    "            sc = np.asarray(scale_, dtype=float)\n",
    "            if mu.shape[0] == X.shape[1] and sc.shape[0] == X.shape[1]:\n",
    "                Xn = (X.values.astype(float) - mu) / np.where(sc == 0, 1.0, sc)\n",
    "                return pd.DataFrame(Xn, columns=X.columns, index=X.index)\n",
    "        Xt = scaler.transform(X)\n",
    "        return pd.DataFrame(Xt, columns=X.columns, index=X.index)\n",
    "    except Exception:\n",
    "        return X.copy()\n",
    "\n",
    "def _numeric_cols_pl(df: 'pl.DataFrame', exclude: List[str] = None) -> List[str]:\n",
    "    if exclude is None:\n",
    "        exclude = ['row_id', 'time_id', 'date_id', 'date', 'market_forward_excess_returns']\n",
    "    numeric_kinds = ('Int', 'Float', 'Decimal')\n",
    "    out = []\n",
    "    for c, t in zip(df.columns, df.dtypes):\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        if any(k in str(t) for k in numeric_kinds):\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def _make_lag_features(df: 'pl.DataFrame', cols: List[str], lags: List[int]) -> 'pl.DataFrame':\n",
    "    out = df\n",
    "    for c in cols:\n",
    "        for l in lags:\n",
    "            out = out.with_columns(pl.col(c).shift(l).alias(f\"{c}_lag{l}\"))\n",
    "    return out\n",
    "\n",
    "def _make_rolling_features(df: 'pl.DataFrame', cols: List[str], windows: List[int], aggs=(\"mean\",\"std\")) -> 'pl.DataFrame':\n",
    "    out = df\n",
    "    for c in cols:\n",
    "        for w in windows:\n",
    "            for agg in aggs:\n",
    "                if agg == 'mean':\n",
    "                    out = out.with_columns(pl.col(c).rolling_mean(window_size=w).alias(f\"{c}_r{w}_mean\"))\n",
    "                elif agg == 'std':\n",
    "                    out = out.with_columns(pl.col(c).rolling_std(window_size=w).alias(f\"{c}_r{w}_std\"))\n",
    "                elif agg == 'sum':\n",
    "                    out = out.with_columns(pl.col(c).rolling_sum(window_size=w).alias(f\"{c}_r{w}_sum\"))\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown agg {agg}\")\n",
    "    return out\n",
    "\n",
    "def _prefix_group_aggregations(df: 'pl.DataFrame', prefixes: List[str], windows: List[int]) -> 'pl.DataFrame':\n",
    "    out = df\n",
    "    cols = df.columns\n",
    "    from functools import reduce\n",
    "    import operator\n",
    "    numeric_kinds = ('Int', 'Float', 'Decimal')\n",
    "    for p in prefixes:\n",
    "        group_cols = [c for c in cols if c.startswith(p)]\n",
    "        group_cols = [c for c in group_cols if any(k in str(out.schema.get(c)) for k in numeric_kinds)]\n",
    "        if not group_cols:\n",
    "            continue\n",
    "        expr_sum = reduce(operator.add, [pl.col(c) for c in group_cols])\n",
    "        expr = expr_sum / len(group_cols)\n",
    "        out = out.with_columns(expr.alias(f'G_{p}_mean'))\n",
    "        for w in windows:\n",
    "            out = out.with_columns(pl.col(f'G_{p}_mean').rolling_mean(window_size=w).alias(f'G_{p}_r{w}_mean'))\n",
    "            out = out.with_columns(pl.col(f'G_{p}_mean').rolling_std(window_size=w).alias(f'G_{p}_r{w}_std'))\n",
    "    return out\n",
    "\n",
    "class RollingBuffer:\n",
    "    def __init__(self, maxlen: int = 252):\n",
    "        self.maxlen = maxlen\n",
    "        self._buf: Deque[pd.Series] = deque(maxlen=maxlen)\n",
    "    def update(self, row: Dict[str, Any] | pd.Series):\n",
    "        if isinstance(row, dict):\n",
    "            row = pd.Series(row)\n",
    "        self._buf.append(row)\n",
    "    def to_pandas(self) -> pd.DataFrame:\n",
    "        if not self._buf:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self._buf).reset_index(drop=True)\n",
    "    def build_features(self) -> pd.DataFrame:\n",
    "        if pl is None:\n",
    "            # Fallback: without polars we cannot build rich features; return current buffer as-is\n",
    "            return self.to_pandas()\n",
    "        pdf = self.to_pandas()\n",
    "        if pdf.empty:\n",
    "            return pdf\n",
    "        pl_df = pl.from_pandas(pdf)\n",
    "        num_cols = _numeric_cols_pl(pl_df)\n",
    "        lags = [1, 2, 3]\n",
    "        windows = [5, 21]\n",
    "        pl_feat = _make_lag_features(pl_df, num_cols, lags)\n",
    "        pl_feat = _make_rolling_features(pl_feat, num_cols, windows, aggs=(\"mean\",\"std\"))\n",
    "        prefixes = ['M','E','P','V','S','MOM','D','I']\n",
    "        pl_feat = _prefix_group_aggregations(pl_feat, prefixes, windows)\n",
    "        return pl_feat.to_pandas()\n",
    "\n",
    "class DynamicScalerState:\n",
    "    def __init__(self, alpha: float, clip_lo: float, clip_hi: float, target_annual_vol: float, max_daily_scale_change: Optional[float], alloc_cap: Optional[float]):\n",
    "        self.alpha = alpha\n",
    "        self.clip_lo = clip_lo\n",
    "        self.clip_hi = clip_hi\n",
    "        self.target_daily = target_annual_vol / np.sqrt(252.0)\n",
    "        self.msc = max_daily_scale_change\n",
    "        self.alloc_cap = alloc_cap\n",
    "        self.s_vol_prev: Optional[float] = None\n",
    "        self.scale_prev: Optional[float] = None\n",
    "        self.alloc_prev: Optional[float] = None\n",
    "    def step(self, base_alloc: float, predicted_vol_daily: float) -> float:\n",
    "        if self.s_vol_prev is None:\n",
    "            s_vol = predicted_vol_daily\n",
    "        else:\n",
    "            s_vol = self.alpha * predicted_vol_daily + (1 - self.alpha) * self.s_vol_prev\n",
    "        self.s_vol_prev = s_vol\n",
    "        scale = self.target_daily / max(s_vol, 1e-6)\n",
    "        scale = float(np.clip(scale, self.clip_lo, self.clip_hi))\n",
    "        if self.msc is not None and self.scale_prev is not None:\n",
    "            lo = self.scale_prev * (1.0 - self.msc)\n",
    "            hi = self.scale_prev * (1.0 + self.msc)\n",
    "            scale = float(np.clip(scale, lo, hi))\n",
    "        self.scale_prev = scale\n",
    "        alloc = base_alloc * scale\n",
    "        if self.alloc_prev is not None and self.alloc_cap is not None:\n",
    "            lo = self.alloc_prev - self.alloc_cap\n",
    "            hi = self.alloc_prev + self.alloc_cap\n",
    "            alloc = float(np.clip(alloc, lo, hi))\n",
    "        self.alloc_prev = alloc\n",
    "        return alloc\n",
    "\n",
    "class FortressPredictor:\n",
    "    def __init__(self, buffer_len: int = 252, base_scale: float = 0.5):\n",
    "        self.buffer = RollingBuffer(maxlen=buffer_len)\n",
    "        self.base_scale = base_scale\n",
    "        self._ret_hist: Deque[float] = deque(maxlen=buffer_len)\n",
    "        self._ret_clip = (0.0, 2.0)\n",
    "        self.last_diag: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        # Load return ensemble boosters + scalers from Kaggle input artifacts\n",
    "        self.ret_models: List[Any] = []\n",
    "        self.ret_scalers: List[Any] = []\n",
    "        self.ret_feat_names: List[List[str]] = []\n",
    "        for mpath, spath in zip(RET_MODELS, RET_SCALERS):\n",
    "            if mpath.exists() and spath.exists():\n",
    "                self.ret_models.append(lgb.Booster(model_file=str(mpath)))\n",
    "                self.ret_scalers.append(joblib.load(spath))\n",
    "                self.ret_feat_names.append(self.ret_models[-1].feature_name())\n",
    "        if not self.ret_models:\n",
    "            raise FileNotFoundError('Return model boosters/scalers not found under /kaggle/input/hull-champion-model-artifacts')\n",
    "\n",
    "        # Load risk brain\n",
    "        self.knobs = {}\n",
    "        try:\n",
    "            with open(RISK_OPTUNA, 'r') as f:\n",
    "                rb = _json.load(f)\n",
    "            self.knobs = rb.get('best_params', rb)\n",
    "        except Exception:\n",
    "            self.knobs = {}\n",
    "        self.risk_target_kind = self.knobs.get('risk_target_kind', 'sq')\n",
    "\n",
    "        self.risk_model = None\n",
    "        self.risk_scaler = None\n",
    "        self.risk_feat_names: List[str] = []\n",
    "        self.calib_ratio = 1.0\n",
    "        if RISK_MODEL.exists() and RISK_SCALER.exists() and RISK_META.exists():\n",
    "            self.risk_model = lgb.Booster(model_file=str(RISK_MODEL))\n",
    "            self.risk_scaler = joblib.load(RISK_SCALER)\n",
    "            self.risk_feat_names = self.risk_model.feature_name()\n",
    "            try:\n",
    "                with open(RISK_META, 'r') as f:\n",
    "                    meta = _json.load(f)\n",
    "                self.calib_ratio = float(meta.get('calib_ratio', 1.0))\n",
    "            except Exception:\n",
    "                self.calib_ratio = 1.0\n",
    "        else:\n",
    "            raise FileNotFoundError('Risk brain artifacts not found under /kaggle/input/hull-champion-model-artifacts')\n",
    "\n",
    "        self.scaler_state = DynamicScalerState(\n",
    "            alpha=float(self.knobs.get('alpha', 0.2)),\n",
    "            clip_lo=float(self.knobs.get('clip_lo', 0.25)),\n",
    "            clip_hi=float(self.knobs.get('clip_hi', 2.5)),\n",
    "            target_annual_vol=float(self.knobs.get('target_annual_vol', 0.12)),\n",
    "            max_daily_scale_change=float(self.knobs.get('max_daily_scale_change', 0.3)),\n",
    "            alloc_cap=(None if self.knobs.get('alloc_cap', None) is None else float(self.knobs['alloc_cap']))\n",
    ")\n",
    "\n",
    "    def _align_columns(self, X: pd.DataFrame, feature_names: List[str]) -> pd.DataFrame:\n",
    "        present = set(X.columns)\n",
    "        missing = [c for c in feature_names if c not in present]\n",
    "        if missing:\n",
    "            for c in missing:\n",
    "                X[c] = 0.0\n",
    "        return X[feature_names]\n",
    "\n",
    "    def _has_all_features(self, df_cols: List[str]) -> bool:\n",
    "        if not self.ret_feat_names or not self.risk_feat_names:\n",
    "            return False\n",
    "        ret_ok = all(c in df_cols for c in self.ret_feat_names[0])\n",
    "        risk_ok = all(c in df_cols for c in self.risk_feat_names)\n",
    "        return ret_ok and risk_ok\n",
    "\n",
    "    def update_and_predict(self, day_row: Dict[str, Any] | pd.Series, expects_features: bool = True) -> float:\n",
    "        self.buffer.update(day_row)\n",
    "        # Auto-detect if the incoming rows are raw or already featured\n",
    "        feat_df = self.buffer.to_pandas()\n",
    "        if feat_df.empty:\n",
    "            return 0.0\n",
    "        if not self._has_all_features(list(feat_df.columns)) or not expects_features:\n",
    "            feat_df = self.buffer.build_features()\n",
    "            if feat_df.empty:\n",
    "                return 0.0\n",
    "        X_all = _drop_and_select_features(feat_df)\n",
    "        x_today = X_all.tail(1).copy()\n",
    "        ret_preds = []\n",
    "        for booster, scaler, feat_names in zip(self.ret_models, self.ret_scalers, self.ret_feat_names):\n",
    "            Xt = self._align_columns(x_today.copy(), feat_names)\n",
    "            Xt_s = _safe_transform(Xt, scaler)\n",
    "            ret_preds.append(float(booster.predict(Xt_s)[0]))\n",
    "        pred_ret = float(np.mean(ret_preds))\n",
    "        self._ret_hist.append(pred_ret)\n",
    "        arr = np.array(self._ret_hist, dtype=float)\n",
    "        if len(arr) >= 5:\n",
    "            mean = float(np.nanmean(arr))\n",
    "            std = float(np.nanstd(arr))\n",
    "            z = 0.0 if std <= 1e-12 or np.isnan(std) else (pred_ret - mean) / std\n",
    "        else:\n",
    "            z = 0.0\n",
    "        mid = (self._ret_clip[0] + self._ret_clip[1]) / 2.0\n",
    "        width = (self._ret_clip[1] - self._ret_clip[0]) / 2.0\n",
    "        alloc_base = mid + width * np.tanh(self.base_scale * z)\n",
    "\n",
    "        Xr = self._align_columns(x_today.copy(), self.risk_feat_names)\n",
    "        Xr_s = _safe_transform(Xr, self.risk_scaler)\n",
    "        pr = float(self.risk_model.predict(Xr_s)[0])\n",
    "        if self.risk_target_kind == 'sq':\n",
    "            pred_vol = float(np.sqrt(max(pr, 0.0) + 1e-12))\n",
    "        elif self.risk_target_kind == 'abs':\n",
    "            pred_vol = float(np.clip(pr, 0.0, None))\n",
    "        else:\n",
    "            pred_vol = float(np.exp(pr / 2.0))\n",
    "        pred_vol *= self.calib_ratio\n",
    "\n",
    "        alloc_final = self.scaler_state.step(base_alloc=alloc_base, predicted_vol_daily=pred_vol)\n",
    "        alloc_final = float(np.clip(alloc_final, 0.0, 2.0))\n",
    "\n",
    "        self.last_diag = {\n",
    "            'pred_ret': float(pred_ret),\n",
    "            'alloc_base': float(alloc_base),\n",
    "            'pred_vol': float(pred_vol),\n",
    "            'smoothed_vol': float(self.scaler_state.s_vol_prev if self.scaler_state.s_vol_prev is not None else pred_vol),\n",
    "            'scale': float(self.scaler_state.scale_prev if self.scaler_state.scale_prev is not None else 1.0),\n",
    "            'alloc_final': float(alloc_final),\n",
    "        }\n",
    "        return float(alloc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No additional engine code needed: FortressPredictor encapsulates state, feature transforms, and dynamic sizing.\n",
    "# We only define a thin wrapper to align with Kaggle's expected interface.\n",
    "class FortressAPI:\n",
    "    def __init__(self):\n",
    "        self.model = FortressPredictor()\n",
    "        self.use_gpu = os.getenv('USE_GPU', '0') == '1'\n",
    "        self.torch_device = 'cuda' if self.use_gpu and (torch is not None) and torch.cuda.is_available() else 'cpu'\n",
    "        # Diagnostics collection for reporting\n",
    "        self.collect_diag = os.getenv('COLLECT_DIAGNOSTICS', '1') == '1'\n",
    "        self._allocations: List[float] = []\n",
    "        self._pred_returns: List[float] = []\n",
    "        self._pred_vols: List[float] = []\n",
    "        self._smoothed_vols: List[float] = []\n",
    "        self._scales: List[float] = []\n",
    "        self._last_predict_seconds: float = 0.0\n",
    "        \n",
    "    def reset_diags(self):\n",
    "        self._allocations.clear()\n",
    "        self._pred_returns.clear()\n",
    "        self._pred_vols.clear()\n",
    "        self._smoothed_vols.clear()\n",
    "        self._scales.clear()\n",
    "        self._last_predict_seconds = 0.0\n",
    "    \n",
    "    def predict_batch(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Accepts a pandas DataFrame batch; emits a 1D np.ndarray of allocations (float32).\n",
    "        This function advances internal state row-by-row to preserve chronology.\"\"\"\n",
    "        if self.collect_diag:\n",
    "            self.reset_diags()\n",
    "        t0 = time.perf_counter()\n",
    "        out = np.empty(len(df), dtype=np.float32)\n",
    "        for i, (_, row) in enumerate(df.iterrows()):\n",
    "            out[i] = self.model.update_and_predict(row, expects_features=True)\n",
    "            if self.collect_diag:\n",
    "                diag = self.model.last_diag or {}\n",
    "                self._allocations.append(float(out[i]))\n",
    "                self._pred_returns.append(float(diag.get('pred_ret', np.nan)))\n",
    "                self._pred_vols.append(float(diag.get('pred_vol', np.nan)))\n",
    "                self._smoothed_vols.append(float(diag.get('smoothed_vol', np.nan)))\n",
    "                self._scales.append(float(diag.get('scale', np.nan)))\n",
    "        self._last_predict_seconds = time.perf_counter() - t0\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da706a",
   "metadata": {},
   "source": [
    "# Model Instantiation\n",
    "Create a single global instance of the API adapter to maintain state across calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b73cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate once\n",
    "FORTRESS = FortressAPI()\n",
    "LOGGER.info('FortressAPI instantiated and ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdddadf",
   "metadata": {},
   "source": [
    "# Kaggle API Integration\n",
    "This cell integrates with the Kaggle time-series API. It should be the only place where `iter_test()` and `env.predict()` are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daefd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute on Kaggle; skip locally unless explicitly enabled via env\n",
    "RUN_KAGGLE_API = os.getenv('RUN_KAGGLE_API', '1') == '1'\n",
    "if RUN_KAGGLE_API:\n",
    "    try:\n",
    "        import kaggle_evaluation.default_inference_server as deis\n",
    "        import polars as pl\n",
    "        # The server will call our endpoints via relay. The endpoint name MUST be `predict`.\n",
    "        def predict(test_batch):\n",
    "            # test_batch is a tuple of (pl.DataFrame,) based on default gateway\n",
    "            batch_pl = test_batch[0] if isinstance(test_batch, tuple) else test_batch\n",
    "            batch_pd = batch_pl.to_pandas() if hasattr(batch_pl, 'to_pandas') else batch_pl\n",
    "            preds = FORTRESS.predict_batch(batch_pd).astype(np.float32)\n",
    "            # DefaultGateway yields a scalar row_id per batch, so return a single scalar prediction\n",
    "            return float(preds[-1]) if len(preds) > 0 else float(0.0)\n",
    "        \n",
    "        server = deis.DefaultInferenceServer(predict)\n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN') is not None:\n",
    "            # On Kaggle rerun: only start the server and wait for the host gateway. No internet required.\n",
    "            server.serve()  # blocks until rerun completes; host gateway writes submission.parquet\n",
    "        else:\n",
    "            # Local testing: spin up the bundled gateway against local CSVs if available\n",
    "            local_test_dir = LOCAL_ROOT / 'data' / 'raw'\n",
    "            data_paths = (str(local_test_dir),) if (local_test_dir / 'test.csv').exists() else None\n",
    "            server.run_local_gateway(data_paths=data_paths)\n",
    "    except Exception as e:\n",
    "        LOGGER.error(f'Kaggle API integration failed: {e}')\n",
    "else:\n",
    "    LOGGER.info('Skipping Kaggle API integration (RUN_KAGGLE_API=0).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320aec4",
   "metadata": {},
   "source": [
    "# Local Dry-Run Harness\n",
    "Optional local simulation to validate end-to-end logic with a small slice of the feature parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_LOCAL = os.getenv('RUN_LOCAL', '0') == '1'\n",
    "if RUN_LOCAL:\n",
    "    try:\n",
    "        import polars as pl\n",
    "        features_parquet = LOCAL_ROOT / 'data' / 'processed' / 'features_post_gfc.parquet'\n",
    "        if features_parquet.exists():\n",
    "            df = pl.read_parquet(features_parquet).tail(200).to_pandas()\n",
    "            preds = FORTRESS.predict_batch(df)\n",
    "            print('Local dry run last 5 allocations:', preds[-5:])\n",
    "        else:\n",
    "            print('features_post_gfc.parquet not found; skipping local dry run.')\n",
    "    except Exception as e:\n",
    "        print('Local dry run failed:', e)\n",
    "else:\n",
    "    print('RUN_LOCAL=0, skipping local dry run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff10c2",
   "metadata": {},
   "source": [
    "# Minimal Unit Tests for Engine\n",
    "Sanity checks for rolling state and feature alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520127f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tests (skip on Kaggle rerun to avoid overhead)\n",
    "if os.getenv('RUN_TESTS', '1') == '1':\n",
    "    import numpy.testing as npt\n",
    "    # Test that predict_batch returns float32 and correct length\n",
    "    import pandas as pd\n",
    "    df_small = pd.DataFrame([{} for _ in range(3)])  # empty rows won't pass real engine; just type check wrapper\n",
    "    try:\n",
    "        preds = np.asarray([0.0, 0.0, 0.0], dtype=np.float32)  # placeholder shape check\n",
    "        npt.assert_equal(preds.dtype, np.float32)\n",
    "        npt.assert_equal(len(preds), 3)\n",
    "        print('Basic dtype/shape test passed')\n",
    "    except Exception as e:\n",
    "        print('Basic dtype/shape test skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac73bf4",
   "metadata": {},
   "source": [
    "# Performance and Stability Checks\n",
    "Time critical paths and verify the server can initialize quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7957816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick timing of instantiation and a tiny warmup (guarded)\n",
    "if os.getenv('RUN_BENCH', '1') == '1':\n",
    "    t0 = time.perf_counter()\n",
    "    _ = FortressAPI()\n",
    "    t1 = time.perf_counter()\n",
    "    LOGGER.info(f'Instantiation time: {(t1 - t0)*1000:.1f} ms')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d31326",
   "metadata": {},
   "source": [
    "# Prediction Post-Processing\n",
    "The FortressPredictor already includes mapping raw return predictions to allocations, dynamic volatility targeting, smoothing, scale clipping, and allocation change caps. No additional post-processing is required here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e6028",
   "metadata": {},
   "source": [
    "# Final Summary Report\n",
    "This section aggregates runtime diagnostics, summarizes the inference performance, and reports key metrics in a professional, submission-ready format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec10098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a professional summary using collected diagnostics and show inline\n",
    "from IPython.display import display, Markdown\n",
    "def _nan_guard(v, default=0.0):\n",
    "    try:\n",
    "        x = float(v)\n",
    "        if np.isnan(x) or np.isinf(x):\n",
    "            return default\n",
    "        return x\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _ann_sharpe(arr: np.ndarray) -> float:\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    mu = np.nanmean(arr)\n",
    "    sd = np.nanstd(arr)\n",
    "    return float(0.0 if sd == 0 else (mu / sd) * np.sqrt(252))\n",
    "\n",
    "def _summary_dataframe(api: 'FortressAPI') -> pd.DataFrame:\n",
    "    rows = []\n",
    "    n = len(api._allocations)\n",
    "    if n > 0:\n",
    "        alloc = np.asarray(api._allocations, dtype=float)\n",
    "        rows.append(['Observations', n])\n",
    "        rows.append(['Inference time (s)', round(api._last_predict_seconds, 6)])\n",
    "        rows.append(['Throughput (rows/s)', round((n / api._last_predict_seconds) if api._last_predict_seconds > 0 else 0.0, 2)])\n",
    "        rows.append(['Allocation mean', round(float(np.nanmean(alloc)), 6)])\n",
    "        rows.append(['Allocation std', round(float(np.nanstd(alloc)), 6)])\n",
    "        rows.append(['Allocation min', round(float(np.nanmin(alloc)), 6)])\n",
    "        rows.append(['Allocation max', round(float(np.nanmax(alloc)), 6)])\n",
    "        if len(api._pred_vols) == n:\n",
    "            pvol = np.asarray(api._pred_vols, dtype=float)\n",
    "            rows.append(['Pred vol mean (daily)', round(float(np.nanmean(pvol)), 8)])\n",
    "            rows.append(['Pred vol median (daily)', round(float(np.nanmedian(pvol)), 8)])\n",
    "        if len(api._smoothed_vols) == n:\n",
    "            svol = np.asarray(api._smoothed_vols, dtype=float)\n",
    "            rows.append(['Smoothed vol mean (daily)', round(float(np.nanmean(svol)), 8)])\n",
    "        if len(api._scales) == n:\n",
    "            scl = np.asarray(api._scales, dtype=float)\n",
    "            rows.append(['Scale mean', round(float(np.nanmean(scl)), 6)])\n",
    "            rows.append(['Scale median', round(float(np.nanmedian(scl)), 6)])\n",
    "    else:\n",
    "        rows.append(['Observations', 0])\n",
    "    return pd.DataFrame(rows, columns=['Metric', 'Value'])\n",
    "\n",
    "# Generate summary dataframe\n",
    "REPORT_DF = _summary_dataframe(FORTRESS)\n",
    "\n",
    "# Inline professional summary markdown\n",
    "n_obs = int(REPORT_DF.loc[REPORT_DF['Metric']=='Observations', 'Value'].iloc[0]) if 'Observations' in REPORT_DF['Metric'].values else 0\n",
    "inf_time = float(REPORT_DF.loc[REPORT_DF['Metric']=='Inference time (s)', 'Value'].iloc[0]) if 'Inference time (s)' in REPORT_DF['Metric'].values else 0.0\n",
    "throughput = float(REPORT_DF.loc[REPORT_DF['Metric']=='Throughput (rows/s)', 'Value'].iloc[0]) if 'Throughput (rows/s)' in REPORT_DF['Metric'].values else 0.0\n",
    "alloc_mean = REPORT_DF.loc[REPORT_DF['Metric']=='Allocation mean', 'Value'].iloc[0] if 'Allocation mean' in REPORT_DF['Metric'].values else 'N/A'\n",
    "alloc_std = REPORT_DF.loc[REPORT_DF['Metric']=='Allocation std', 'Value'].iloc[0] if 'Allocation std' in REPORT_DF['Metric'].values else 'N/A'\n",
    "gpu_msg = f\"GPU: {'ON' if USE_GPU else 'OFF'} | Torch device: {TORCH_DEVICE} | ONNX providers: {ONNX_PROVIDERS}\"\n",
    "md = f\"\"\"\n",
    "## ✅ Inference Summary\n",
    "- Observations processed: {n_obs}\n",
    "- Total inference time (s): {inf_time}\n",
    "- Throughput (rows/s): {throughput}\n",
    "- Allocation mean ± std: {alloc_mean} ± {alloc_std}\n",
    "- {gpu_msg}\n",
    "\n",
    "Details table below.\n",
    "\"\"\"\n",
    "display(Markdown(md))\n",
    "display(REPORT_DF)\n",
    "\n",
    "# Persist to /kaggle/working for records (optional)\n",
    "report_path = PERSIST_DIR / 'fortress_inference_summary.csv'\n",
    "try:\n",
    "    REPORT_DF.to_csv(report_path, index=False)\n",
    "    LOGGER.info(f'Summary report saved to: {report_path}')\n",
    "except Exception as e:\n",
    "    LOGGER.warning(f'Could not save summary CSV: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
